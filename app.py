import streamlit as st
import os
import langchain 
st.write("LangChain version:", langchain.__version__)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# --- 1. CONFIGURA√á√ÉO DE SEGURAN√áA ---
# No Streamlit Cloud, coloque a chave em Settings -> Secrets
if "GOOGLE_API_KEY" in st.secrets:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
else:
    st.error("‚ö†Ô∏è ERRO: Configure a 'GOOGLE_API_KEY' nos Secrets do Streamlit.")
    st.stop()

# --- 2. MOTOR DE INTELIG√äNCIA (RAG) ---
@st.cache_resource
def carregar_base_conhecimento():
    """L√™ os arquivos na pasta 'legislacao' e cria o banco de dados da IA."""
    if not os.path.exists("legislacao"):
        os.makedirs("legislacao")
        
    # Carrega PDFs e DOCX
    pdf_loader = DirectoryLoader('legislacao/', glob="./*.pdf", loader_cls=PyPDFLoader)
    docx_loader = DirectoryLoader('legislacao/', glob="./*.docx", loader_cls=Docx2txtLoader)
    
    docs = pdf_loader.load() + docx_loader.load()
    
    if not docs:
        return None

    # Divide o texto em blocos para a IA n√£o se perder
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=250,
        separators=["\nArt.", "\n¬ß", "\n\n", "\n"]
    )
    chunks = text_splitter.split_documents(docs)

    # Cria o banco de vetores (Embeddings do Gemini)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = FAISS.from_documents(chunks, embeddings)
    return vector_store

# --- 3. DEFINI√á√ÉO DO SYSTEM PROMPT (O "C√âREBRO" DO CONSULTOR) ---
system_template = """
Voc√™ √© o Consultor Legislativo S√™nior da C√¢mara Municipal de Espumoso, RS.
Sua miss√£o √© fornecer an√°lises t√©cnicas fundamentadas na Lei Org√¢nica, Regimento Interno e Regime Jur√≠dico.

DIRETRIZES:
1. LEGALIDADE: Use linguagem formal e cite sempre o Artigo, Par√°grafo ou Inciso (ex: Art. 12, ¬ß1¬∫).
2. CONTEXTO: Utilize APENAS os documentos fornecidos para responder. 
3. SE N√ÉO SOUBER: Se a lei n√£o mencionar o assunto, diga: "N√£o localizei previs√£o espec√≠fica na legisla√ß√£o dispon√≠vel."
4. ESTILO: Seja pragm√°tico, direto e t√©cnico.

CONTEXTO DOS DOCUMENTOS:
{context}

HIST√ìRICO DA CONVERSA:
{chat_history}
"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]
qa_prompt = ChatPromptTemplate.from_messages(messages)

# --- 4. INTERFACE DO USU√ÅRIO (STREAMLIT) ---
st.set_page_config(page_title="IA Legislativa Espumoso", page_icon="üèõÔ∏è")
st.title("üèõÔ∏è Consultor Legislativo Digital")
st.subheader("C√¢mara Municipal de Espumoso/RS")

# Inicializa o Banco de Dados
vector_db = carregar_base_conhecimento()

if vector_db is None:
    st.info("üìå Por favor, adicione os arquivos PDF na pasta 'legislacao' para come√ßar.")
    st.stop()

# Inicializa Mem√≥ria e Chain
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True, 
        output_key='answer'
    )

llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.1)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_db.as_retriever(search_kwargs={"k": 4}),
    memory=st.session_state.memory,
    combine_docs_chain_kwargs={"prompt": qa_prompt},
    return_source_documents=True
)

# Chat UI
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Em que posso ajudar hoje?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Consultando legisla√ß√£o..."):
            response = qa_chain.invoke({"question": prompt})
            answer = response['answer']
            
            # Extrai as fontes para transpar√™ncia
            sources = set([os.path.basename(doc.metadata['source']) for doc in response['source_documents']])
            source_text = f"\n\n---\n**Fontes:** {', '.join(sources)}"
            
            full_response = answer + source_text
            st.markdown(full_response)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

